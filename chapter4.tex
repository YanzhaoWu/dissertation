\chapter{Sparse matrix multiplication}
\label{sec:fe}
\chaptermark{}

Owing to random memory access patterns, sparse matrix multiplication is
traditionally performed in memory and scales to large matrices using
the distributed memory of multiple nodes.
In contrast, we scale sparse matrix multiplication by utilizing commodity
SSDs. We implement sparse matrix dense matrix multiplication (SpMM) in
a semi-external memory (SEM) fashion, i.e., we keep the sparse matrix on SSDs
and dense matrices in memory. Our SEM SpMM can incorporate many
in-memory optimizations for large power-law graphs with near-random vertex
connection. Coupled with many I/O optimizations, our SEM SpMM achieves performance
comparable to our in-memory implementation on a large parallel machine and
outperforms the implementations in Trilinos and Intel MKL.
Our experiments show that the SEM SpMM achieves almost 100\% performance
of the in-memory implementation on graphs when the dense matrix has more
than four columns; it achieves at least 65\% performance of the in-memory
implementation for all of our graphs when the dense matrix has only one column.
We apply our SpMM to three important data analysis applications and show that
our SSD-based implementations can significantly outperform state of the art
of these applications and scale to billion-node graphs.

\section{Introduction}
\input{SpMM/intro}

\section{Related Work}
\input{SpMM/relwork}

\input{SpMM/design}

\input{SpMM/eval}

\section{Conclusions}
\input{SpMM/conclusion}
