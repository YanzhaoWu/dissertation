\chapter{Sparse matrix multiplication}
\label{sec:fe}
\chaptermark{Semi-External Memory Sparse Matrix Multiplication on
Billion-node Graphs in a Multicore Architecture}

This chapter describes another view of graph analysis. Instead of viewing
a graph as a collection of vertices and edges, we encode a graph as
a sparse matrix and express graph analysis as matrix operations \cite{Mattson13}.
In this formulation, a row or a column of a sparse matrix represents a vertex
in a graph and a non-zero entry encodes the existence of an edge or the edge
weight on a graph. As such, many graph analysis algorithms such as PageRank
and spectral clustering are expressed as sparse matrix multiplication.
In this chapter, we describe an efficient implementation of sparse matrix
dense matrix multiplication (SpMM). We scale this matrix operation by utilizing
commodity SSDs and implement it in
a semi-external memory (SEM) fashion, i.e., we keep the sparse matrix on SSDs
and dense matrices in memory. Our SEM SpMM incorporates many
in-memory optimizations for large power-law graphs with near-random vertex
connection. Coupled with many I/O optimizations, our SEM SpMM achieves performance
comparable to our in-memory implementation on a large parallel machine and
outperforms the implementations in Trilinos and Intel MKL.
Our experiments show that the SEM SpMM achieves almost 100\% performance
of the in-memory implementation on graphs when the dense matrix has more
than four columns; it achieves at least 65\% performance of the in-memory
implementation for all of our graphs when the dense matrix has only one column.
We apply our SpMM to three important data analysis applications and show that
our SSD-based implementations can significantly outperform state of the art
of these applications and scale to billion-node graphs.

\section{Introduction}
\input{SpMM/intro}

\section{Related Work}
\input{SpMM/relwork}

\input{SpMM/design}

\input{SpMM/eval}

\section{Conclusions}
\input{SpMM/conclusion}
